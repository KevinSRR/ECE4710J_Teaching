{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RC I\n",
    "---\n",
    "ECE4710J  2022SP\n",
    "\n",
    "Materials collected by Sizhe Zhou. Credit to all the related online resources and the usage of this notebook is limited to education purpose. \n",
    "\n",
    "Feb. 25th, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Python\n",
    "\n",
    "## 1.1 with statement\n",
    "\n",
    "with statement in Python is used in exception handling to make the code cleaner and much more readable. It simplifies the management of common resources like file streams. Observe the following code example on how the use of with statement makes code cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file handling\n",
    "  \n",
    "# 1) without using with statement\n",
    "file = open('file_path', 'w')\n",
    "file.write('hello world !')\n",
    "file.close()\n",
    "  \n",
    "# 2) without using with statement\n",
    "file = open('file_path', 'w')\n",
    "try:\n",
    "    file.write('hello world')\n",
    "finally:\n",
    "    file.close()\n",
    "\n",
    "\n",
    "# using with statement\n",
    "with open('file_path', 'w') as file:\n",
    "    file.write('hello world !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that unlike the first two implementations, there is no need to call `file.close()` when using with statement. The with statement itself ensures proper acquisition and release of resources. An exception during the `file.write()` call in the first implementation can prevent the file from closing properly which may introduce several bugs in the code, i.e. many changes in files do not go into effect until the file is properly closed.\n",
    "\n",
    "The second approach in the above example takes care of all the exceptions but using the with statement makes the code compact and much more readable. Thus, with statement helps avoiding bugs and leaks by ensuring that a resource is properly released when the code using the resource is completely executed. The with statement is popularly used with file streams, as shown above and with Locks, sockets, subprocesses and telnets etc.\n",
    "\n",
    ">More advanced usage: context Manager; contextlib module...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Slicing (Similar with Numpy)\n",
    "List slicing returns a new list from the existing list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [1,2,3]\n",
    "print('lst: ', id(lst))\n",
    "\n",
    "# be careful\n",
    "lst1 = lst\n",
    "print('lst1: ', id(lst))\n",
    "\n",
    "lst2 = lst[:2]\n",
    "print('lst2: ', id(lst2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax: `Lst[ Initial : End : IndexJump ]`\n",
    "\n",
    "If Lst is a list, then the above expression returns the portion of the list from index Initial to index End, at a step size IndexJump.\n",
    "If Initial is not assigned, it indexs from 0. If End is not assigned, it indexs till the end. The default value for IndexJump is 1.\n",
    "> note: if IndexJump is 1, the length can be directly calculated by End - Initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Lambda Function\n",
    "\n",
    "Synatx: `lambda_expr ::=  \"lambda\" [parameter_list] \":\" expression`\n",
    "\n",
    "\n",
    "Lambda expressions (sometimes called lambda forms) are used to create anonymous functions. The expression lambda parameters: expression yields a function object. The unnamed object behaves like a function object defined with:\n",
    "\n",
    "```python\n",
    "\n",
    "def <lambda>(parameters):\n",
    "    return expression\n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "Note that functions created with lambda expressions cannot contain statements or annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 10 to argument a, and return the result:\n",
    "x = lambda a: a + 10\n",
    "print(x(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply argument a with argument b and return the result:\n",
    "x = lambda a, b : a * b\n",
    "print(x(5, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Use Lambda Functions?\n",
    "The power of lambda is better shown when you use them as an anonymous function inside another function.\n",
    "\n",
    "Say you have a function definition that takes one argument, and that argument will be multiplied with an unknown number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(n):\n",
    "  return lambda a : a * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(n):\n",
    "  return lambda a : a * n\n",
    "\n",
    "mydoubler = myfunc(2)\n",
    "\n",
    "print(mydoubler(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(n):\n",
    "  return lambda a : a * n\n",
    "\n",
    "mydoubler = myfunc(2)\n",
    "mytripler = myfunc(3)\n",
    "\n",
    "print(mydoubler(11))\n",
    "print(mytripler(11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use lambda functions when an anonymous function is required for a short period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Generators\n",
    "First lets understand iterators. According to Wikipedia, an iterator is an object that enables a programmer to traverse a container, particularly lists. However, an iterator performs traversal and gives access to data elements in a container, but does not perform iteration. There are three parts namely:\n",
    "\n",
    "- Iterable\n",
    "- Iterator\n",
    "- Iteration\n",
    "\n",
    "### 1.4.1 Iterable\n",
    "An iterable is any object in Python which has an __iter__ or a __getitem__ method defined which returns an iterator or can take indexes. In short an iterable is any object which can provide us with an iterator. So what is an iterator?\n",
    "\n",
    "### 1.4.2 Iterator\n",
    "An iterator is any object in Python which has a next (Python2) or __next__ method defined. That’s it. That’s an iterator. Now let’s understand iteration.\n",
    "\n",
    "### 1.4.3 Iteration\n",
    "In simple words it is the process of taking an item from something e.g a list. When we use a loop to loop over something it is called iteration. It is the name given to the process itself. Now as we have a basic understanding of these terms let’s understand generators.\n",
    "\n",
    "### 1.4.4 Generators\n",
    "Generators are iterators, but you can only iterate over them once. It’s because they do not store all the values in memory, they generate the values on the fly. You use them by iterating over them, either with a ‘for’ loop or by passing them to any function or construct that iterates. Most of the time generators are implemented as functions. However, they do not return a value, they yield it. Here is a simple example of a generator function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_function():\n",
    "    for i in range(10):\n",
    "        yield i\n",
    "\n",
    "for item in generator_function():\n",
    "    print(item)\n",
    "\n",
    "# Output: 0\n",
    "# 1\n",
    "# 2\n",
    "# 3\n",
    "# 4\n",
    "# 5\n",
    "# 6\n",
    "# 7\n",
    "# 8\n",
    "# 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not really useful in this case. Generators are best for calculating large sets of results (particularly calculations involving loops themselves) where you don’t want to allocate the memory for all results at the same time. Many Standard Library functions that return lists in Python 2 have been modified to return generators in Python 3 because generators require fewer resources.\n",
    "\n",
    "Here is an example generator which calculates fibonacci numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator version\n",
    "def fibon(n):\n",
    "    a = b = 1\n",
    "    for i in range(n):\n",
    "        yield a\n",
    "        a, b = b, a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in fibon(10):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we would not have to worry about it using a lot of resources. However, if we would have implemented it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibon(n):\n",
    "    a = b = 1\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append(a)\n",
    "        a, b = b, a + b\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would have used up all our resources while calculating a large input. We have discussed that we can iterate over generators only once but we haven’t tested it. Before testing it you need to know about one more built-in function of Python, next(). It allows us to access the next element of a sequence. So let’s test out our understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_function():\n",
    "    for i in range(3):\n",
    "        yield i\n",
    "\n",
    "gen = generator_function()\n",
    "print(next(gen))\n",
    "# Output: 0\n",
    "print(next(gen))\n",
    "# Output: 1\n",
    "print(next(gen))\n",
    "# Output: 2\n",
    "print(next(gen))\n",
    "# Output: Traceback (most recent call last):\n",
    "#            File \"<stdin>\", line 1, in <module>\n",
    "#         StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that after yielding all the values next() caused a StopIteration error. Basically this error informs us that all the values have been yielded. You might be wondering why we don’t get this error when using a for loop? Well the answer is simple. The for loop automatically catches this error and stops calling next. Did you know that a few built-in data types in Python also support iteration? Let’s check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"Yasoob\"\n",
    "next(my_string)\n",
    "# Output: Traceback (most recent call last):\n",
    "#      File \"<stdin>\", line 1, in <module>\n",
    "#    TypeError: str object is not an iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that’s not what we expected. The error says that str is not an iterator. Well it’s right! It’s an iterable but not an iterator. This means that it supports iteration but we can’t iterate over it directly. So how would we iterate over it? It’s time to learn about one more built-in function, iter. It returns an iterator object from an iterable. While an int isn’t an iterable, we can use it on string!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"Yasoob\"\n",
    "my_iter = iter(my_string)\n",
    "print(next(my_iter))\n",
    "# Output: 'Y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Zip and Unzip\n",
    "\n",
    "### 1.5.1 Zip\n",
    "Let’s say that we have two lists, one that includes first names, and the other includes last names. We would like to somehow combine the first names with the corresponding last names as tuples. In other words, we would like to combine elements from multiple iterables that have the same index together in a list of tuples:\n",
    "\n",
    "```\n",
    "list_1 = ['Jane', 'John', 'Jennifer']\n",
    "list_2 = ['Doe', 'Williams', 'Smith']\n",
    "Desired Output:\n",
    "[('Jane', 'Doe'), ('John', 'Williams'), ('Jennifer', 'Smith')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can accomplish this with the zip() function, which is a built-in python function. The zip() function is named due to its analogous mechanism as physical zippers. When you zip something, you bring both sides together. And that’s how the zip() function works! It brings elements of the same index from multiple iterable objects together as elements of the same tuples.\n",
    "\n",
    "Syntax: `zip(*iterables)`\n",
    "\n",
    "The zip() function takes in iterables as arguments, such as lists, files, tuples, sets, etc. The zip() function will then create an iterator that aggregates elements from each of the iterables passed in. In other words, it will return an iterator of tuples, where the i-th tuple will contain the i-th element from each of the iterables passed in. This iterator will stop once the shortest input iterable has been exhausted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the zip() function\n",
    "first_names = ['Jane', 'John', 'Jennifer']\n",
    "last_names = ['Doe', 'Williams', 'Smith']\n",
    "full_names = list(zip(first_names, last_names))\n",
    "print(full_names)\n",
    "# [('Jane', 'Doe'), ('John', 'Williams'), ('Jennifer', 'Smith')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the zip() function returns an iterator. Thus, we need to use the list() function that will use this returned iterator (or zip object) to create a list. In addition, as long as the iterables passed in are ordered (sequences), then the tuples will contain elements in the same left-to-right order of the arguments passed in the zip() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we have three iterable objects?\n",
    "first_names = ['Jane', 'John', 'Jennifer']\n",
    "last_names = ['Doe', 'Williams', 'Smith']\n",
    "ages = [20, 40, 30]\n",
    "names_and_ages = list(zip(first_names, last_names, ages))\n",
    "print(names_and_ages)\n",
    "# [('Jane', 'Doe', 20), ('John', 'Williams', 40), ('Jennifer', 'Smith', 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing in one argument to zip()\n",
    "# If we only pass in one iterable object to the zip() function, then we will get a list of 1-item tuples as follows:\n",
    "first_names = ['Jane', 'John', 'Jennifer']\n",
    "print(list(zip(first_names)))\n",
    "# [('Jane',), ('John',), ('Jennifer',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterables with unequal lengths\n",
    "first_names = ['Jane', 'John', 'Jennifer']\n",
    "last_names = ['Doe', 'Williams', 'Smith', 'Jones']\n",
    "full_names = list(zip(first_names, last_names))\n",
    "print(full_names)\n",
    "# [('Jane', 'Doe'), ('John', 'Williams'), ('Jennifer', 'Smith')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the elements in the longer iterables are needed, then we can use the itertools.zip_longest() (zip_longest() function located in the itertools module) function instead of zip(). It will continue until the longest iterable is exhausted, and will replace any missing values with the value passed in for the fillvalue argument (default is None)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Iteration of Iterables\n",
    "We can use the zip() function to iterate in parallel over multiple iterables. Since the zip() function returns an iterator, we can use this zip object (the iterator it returns) in a for loop. And since with each iteration of this iterator a tuple is returned, we can unpack the elements of this tuple within the for loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_names = ['Jane', 'John', 'Jennifer']\n",
    "last_names = ['Doe', 'Williams', 'Smith']\n",
    "ages = [20, 40, 30]\n",
    "for first, last, age in zip(first_names, last_names, ages):\n",
    "    print(f'{first} {last} is {age} years old')\n",
    "# Output: \n",
    "# Jane Doe is 20 years old\n",
    "# John Williams is 40 years old\n",
    "# Jennifer Smith is 30 years old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Unzip\n",
    "Let’s say that we have the following list of tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_and_last_names = [('Jane', 'Doe'), ('John', 'Williams'), ('Jennifer', 'Smith')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we want to separate the elements in these tuples into two separate lists. Well, since that is the opposite of zipping (bringing things together), it would be unzipping (taking things apart). To unzip in python, we can use the unpacking operator * with the zip() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_names, last_names = zip(*first_and_last_names)\n",
    "first_names = list(first_names)\n",
    "last_names = list(last_names)\n",
    "print(first_names)\n",
    "# ['Jane', 'John', 'Jennifer']\n",
    "print(last_names)\n",
    "# ['Doe', 'Williams', 'Smith']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unpacking operator * will unpack the first_and_last_names list of tuples into its tuples. These tuples will then be passed to the zip() function, which will take these separate iterable objects (the tuples), and combines their same-indexed elements together into tuples, making two separate tuples. Lastly, through tuple unpacking, these separated tuples will be assigned to the first_names and last_names variables. We then use the list() function to convert these tuples into lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Unpacking Operators in Python\n",
    "### 1.6.1 * Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s say we have a list:\n",
    "num_list = [1,2,3,4,5]\n",
    "\n",
    "# And we define a function that takes in 5 arguments and returns their sum:\n",
    "def num_sum(num1,num2,num3,num4,num5):\n",
    "    return num1 + num2 + num3 + num4 + num5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we want to find the sum of all the elements in num_list. Well, we can accomplish this by passing in all the elements of num_list to the function num_sum. Since num_list has five elements in it, the num_sum function contains five parameters, one for each element in num_list.\n",
    "One way to do this would be to pass the elements by using their index as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sum(num_list[0], num_list[1], num_list[2], num_list[3], num_list[4])\n",
    "# 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a much easier way to do this, and that’s by using the * operator. The * operator is an unpacking operator that will unpack the values from any iterable object, such as lists, tuples, strings, etc…\n",
    "For example, if we want to unpack num_list and pass in the 5 elements as separate arguments for the num_sum function, we could do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sum(*num_list)\n",
    "# 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that’s it! The asterisk, *, or unpacking operator, unpacks num_list, and passes the values, or elements, of num_list as separate arguments to the num_sum function.\n",
    "\n",
    "Note: For this to work, the number of elements in num_list must match the number of parameters in the num_sum function. If they don’t match, we would get a TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Operator with Built-In Functions:\n",
    "# We can also use the asterisk, *, or unpacking operator, with built-in functions in python, such as print:\n",
    "print(*num_list)\n",
    "# 1 2 3 4 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpacking Multiple Lists:\n",
    "# Let’s say we have another list:\n",
    "num_list_2 = [6,7,8,9,10]\n",
    "\n",
    "# And we want to print all the elements in both num_list and num_list_2. We can use the unpacking operator, *, to accomplish this as follows:\n",
    "print(*num_list, *num_list_2)\n",
    "# 1 2 3 4 5 6 7 8 9 10\n",
    "# Both num_list and num_list_2 are unpacked. Then, all the elements are passed in to print as separate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Multiple Lists:\n",
    "# We can also create a new list that contains all the elements from num_list and num_list_2:\n",
    "new_list = [*num_list, *num_list_2]\n",
    "# [1,2,3,4,5,6,7,8,9,10]\n",
    "# Note: We could have simply added num_list and num_list_2 to create new_list. However, this was just to portray the functionality of the unpacking operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Uses of * Operator:\n",
    "# Let’s say that we have a string assigned to the variable name:\n",
    "name = 'Michael'\n",
    "\n",
    "# And we want to break this name up into 3 parts, with the first letter being assigned to a variable, \n",
    "# the last letter being assigned to another variable, \n",
    "# and everything in the middle assigned to a third variable. We can do so as follows:\n",
    "# first, *middle, last = name\n",
    "first, *middle, last = name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that’s it! Since name is a string, and strings are iterable objects, we can unpack them. The values on the right side of the assignment operator will be assigned to the variables on the left depending on their relative position in the iterable object. As such, the first letter of ‘Michael’ is assigned to the variable first, which would be ‘M’ in this case. The last letter, ‘l’, is assigned to the variable last. And the variable middle will contain all the letters between ‘M’ and ‘l’ in the form of a list: [‘i’, ‘c’, ‘h’, ‘a’, ‘e’].\n",
    "\n",
    "Note: The first and last variables above are called mandatory variables, as they must be assigned concrete values. The middle variable, due to using the * or unpacking operator, can have any number of values, including zero. If there are not enough values to unpack for the mandatory variables, we will get a ValueError."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 Packing with * Operator:\n",
    "We can also use the * operator to pack multiple values into a single variable. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*names, = 'Michael', 'John', 'Nancy'\n",
    "# names \n",
    "['Michael', 'John', 'Nancy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for using a trailing comma after *names is because the left side of the assignment must be a tuple or list. Therefore, the names variable now contains all the names on the right side in the form of a list.\n",
    "\n",
    "Note: This is what we do when we define functions that can receive a varying number of arguments! That is the concept of *args and **kwargs!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *args:\n",
    "For example, let’s say we have a function, names_tuple, that takes in names as arguments and returns them back. However, the number of names that we pass in to this function can vary. Well, we can’t just choose a number of parameters that this function would have since the number of positional arguments can change with each calling of the function. We can instead use the * operator to pack the arguments passed in into a tuple as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_tuple(*args):\n",
    "    return args\n",
    "names_tuple('Michael', 'John', 'Nancy')\n",
    "# ('Michael', 'John', 'Nancy')\n",
    "names_tuple('Jennifer', 'Nancy')\n",
    "# ('Jennifer', 'Nancy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter what number of positional arguments we pass in when we call the names_tuple function, the *args argument will pack the positional arguments into a tuple, similar to the *names assignment above.\n",
    "\n",
    "#### **kwargs\n",
    "To pass in a varying number of keyword or named arguments, we use the ** operator when defining a function. The ** unpacking operator will pack the varying number of named arguments we pass in into a dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_dict(**kwargs):\n",
    "    return kwargs\n",
    "names_dict(Jane = 'Doe')\n",
    "# {'Jane': 'Doe'}\n",
    "names_dict(Jane = 'Doe', John = 'Smith')\n",
    "# {'Jane': 'Doe', 'John': 'Smith'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionaries\n",
    "What happens when we try to use the * operator with a dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dict = {'a': 1, 'b': 2, 'c': 3}\n",
    "print(*num_dict)\n",
    "# a b c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how it printed the keys of the dictionary and not the values? To unpack a dictionary, we need to use the ** unpacking operator. However, since each value is associated with a specific key, the function that we pass these arguments to must have parameters with the same names as the keys of the dictionary being unpacked. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_sum(a,b,c):\n",
    "    return a+b+c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dict_sum function has three parameters: a, b, and c. These three parameters are named the same as the keys of num_dict. Therefore, once we pass in the unpacked dictionary using the ** operator, it’ll assign in the values of the keys according to the corresponding parameter names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sum(**num_dict)\n",
    "# 6\n",
    "# Thus, the values, or arguments, for the a, b, and c parameters in dict_sum will be 1, 2, and 3, respectively. \n",
    "# And the sum of these three values is 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Dictionaries:\n",
    "Just like with lists, the ** operator can be used to merge two or more dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dict = {'a': 1, 'b': 2, 'c': 3}\n",
    "num_dict_2 = {'d': 4, 'e': 5, 'f': 6}\n",
    "new_dict = {**num_dict, **num_dict_2}\n",
    "new_dict\n",
    "# {‘a’: 1, ‘b’: 2, ‘c’: 3, ‘d’: 4, ‘e’: 5, ‘f’: 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Comprehension\n",
    "Comprehensions are constructs that allow sequences to be built from other sequences. Python 2.0 introduced list comprehensions and Python 3.0 comes with dictionary and set comprehensions.\n",
    "\n",
    "### 1.7.1 List Comprehensions\n",
    "A list comprehension consists of the following parts:\n",
    "\n",
    "- An Input Sequence.\n",
    "- A Variable representing members of the input sequence.\n",
    "- An Optional Predicate expression.\n",
    "- An Output Expression producing elements of the output list from members of the Input Sequence that satisfy the predicate.\n",
    "\n",
    "\n",
    "Say we need to obtain a list of all the integers in a sequence and then square them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = [1, '4', 9, 'a', 0, 4]\n",
    "\n",
    "squared_ints = [e**2 for e in a_list if type(e) == int]\n",
    "\n",
    "print(squared_ints)\n",
    "# [ 1, 81, 0, 16 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./assets/listComprehension.gif' style='zoom: 150%'/>\n",
    "\n",
    "The iterator part iterates through each member e of the input sequence a_list.\n",
    "The predicate checks if the member is an integer.\n",
    "If the member is an integer then it is passed to the output expression, squared, to become a member of the output list.\n",
    "\n",
    "\n",
    "Much the same results can be achieved using the built in functions, map, filter and the anonymous lambda function.\n",
    "\n",
    "The filter function applies a predicate to a sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(lambda e: type(e) == int, a_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map modifies each member of a sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map(lambda e: e**2, a_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two can be combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map(lambda e: e**2, filter(lambda e: type(e) == types.IntType, a_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example involves function calls to map, filter, type and two calls to lambda. Function calls in Python are expensive. Furthermore the input sequence is traversed through twice and an intermediate list is produced by filter.\n",
    "\n",
    "The list comprehension is enclosed within a list so, it is immediately evident that a list is being produced. There is only one function call to type and no call to the cryptic lambda instead the list comprehension uses a conventional iterator, an expression and an if expression for the optional predicate.\n",
    "\n",
    "### 1.7.2 Nested Comprehensions\n",
    "![Matrix](./assets/idMatrix.png)\n",
    "\n",
    "In python we can represent such a matrix by a list of lists, where each sub-list represents a row. A 3 by 3 matrix would be represented by the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ [ 1 if item_idx == row_idx else 0 for item_idx in range(0, 3) ] for row_idx in range(0, 3) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.3 Techniques\n",
    "Using zip() and dealing with two or more elements at a time:\n",
    "\n",
    "```\n",
    "['%s=%s' % (n, v) for n, v in zip(self.all_names, self)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple types (auto unpacking of a tuple):\n",
    "```\n",
    "[f(v) for (n, f), v in zip(cls.all_slots, values)]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A two-level list comprehension using os.walk():\n",
    "```\n",
    "# Comprehensions/os_walk_comprehension.py\n",
    "import os\n",
    "restFiles = [os.path.join(d[0], f) for d in os.walk(\".\")\n",
    "             for f in d[2] if f.endswith(\".rst\")]\n",
    "for r in restFiles:\n",
    "    print(r)\n",
    "```\n",
    "\n",
    "os.walk()'s signature:\n",
    "`walk(top, topdown=True, onerror=None, followlinks=False)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.4 Set Comprehensions\n",
    "Set comprehensions allow sets to be constructed using the same principles as list comprehensions, the only difference is that resulting sequence is a set.\n",
    "\n",
    "Say we have a list of names. The list can contain names which only differ in the case used to represent them, duplicates and names consisting of only one character. We are only interested in names longer then one character and wish to represent all names in the same format: The first letter should be capitalised, all other characters should be lower case.\n",
    "\n",
    "Given the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [ 'Bob', 'JOHN', 'alice', 'bob', 'ALICE', 'J', 'Bob' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require the set:\n",
    "{ 'Bob', 'John', 'Alice' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the new syntax for denoting a set. Members are enclosed in curly braces.\n",
    "\n",
    "The following set comprehension accomplishes this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{ name[0].upper() + name[1:].lower() for name in names if len(name) > 1 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.5 Dictionary Comprehensions\n",
    "Say we have a dictionary the keys of which are characters and the values of which map to the number of times that character appears in some text. The dictionary currently distinguishes between upper and lower case characters.\n",
    "\n",
    "We require a dictionary in which the occurrences of upper and lower case characters are combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcase = {'a':10, 'b': 34, 'A': 7, 'Z':3}\n",
    "\n",
    "mcase_frequency = { k.lower() : mcase.get(k.lower(), 0) + mcase.get(k.upper(), 0) for k in mcase.keys() }\n",
    "\n",
    "# mcase_frequency == {'a': 17, 'z': 3, 'b': 34}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Pipeline\n",
    "We will not talk about this in this RC. But it will probably be covered by your project. Given all the previous knowledge, you will easily understand this concept and I also don't want to keep rob you of the pleasure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Decorator (Maybe useless for you temporarily)\n",
    "Decorators are a significant part of Python. In simple words: they are functions which modify the functionality of other functions. They help to make our code shorter and more Pythonic. Most beginners do not know where to use them so I am going to share some areas where decorators can make your code more concise.\n",
    "\n",
    "First, let’s discuss how to write your own decorator.\n",
    "\n",
    "It is perhaps one of the most difficult concepts to grasp. We will take it one step at a time so that you can fully understand it.\n",
    "\n",
    "### 1.9.1 Everything in Python is an object:\n",
    "First of all let’s understand functions in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hi(name=\"yasoob\"):\n",
    "    return \"hi \" + name\n",
    "\n",
    "print(hi())\n",
    "# output: 'hi yasoob'\n",
    "\n",
    "# We can even assign a function to a variable like\n",
    "greet = hi\n",
    "# We are not using parentheses here because we are not calling the function hi\n",
    "# instead we are just putting it into the greet variable. Let's try to run this\n",
    "\n",
    "print(greet())\n",
    "# output: 'hi yasoob'\n",
    "\n",
    "# Let's see what happens if we delete the old hi function!\n",
    "del hi\n",
    "print(hi())\n",
    "#outputs: NameError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(greet())\n",
    "#outputs: 'hi yasoob'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9.2. Defining functions within functions:\n",
    "So those are the basics when it comes to functions. Let’s take your knowledge one step further. In Python we can define functions inside other functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hi(name=\"yasoob\"):\n",
    "    print(\"now you are inside the hi() function\")\n",
    "\n",
    "    def greet():\n",
    "        return \"now you are in the greet() function\"\n",
    "\n",
    "    def welcome():\n",
    "        return \"now you are in the welcome() function\"\n",
    "\n",
    "    print(greet())\n",
    "    print(welcome())\n",
    "    print(\"now you are back in the hi() function\")\n",
    "\n",
    "hi()\n",
    "#output:now you are inside the hi() function\n",
    "#       now you are in the greet() function\n",
    "#       now you are in the welcome() function\n",
    "#       now you are back in the hi() function\n",
    "\n",
    "# This shows that whenever you call hi(), greet() and welcome()\n",
    "# are also called. However the greet() and welcome() functions\n",
    "# are not available outside the hi() function e.g:\n",
    "\n",
    "greet() # dont run the above code block if you want it to output right :)\n",
    "#outputs: NameError: name 'greet' is not defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know that we can define functions in other functions. In other words: we can make nested functions. Now you need to learn one more thing, that functions can return functions too.\n",
    "\n",
    "### 1.9.3. Returning functions from within functions:\n",
    "It is not necessary to execute a function within another function, we can return it as an output as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hi(name=\"yasoob\"):\n",
    "    def greet():\n",
    "        return \"now you are in the greet() function\"\n",
    "\n",
    "    def welcome():\n",
    "        return \"now you are in the welcome() function\"\n",
    "\n",
    "    if name == \"yasoob\":\n",
    "        return greet\n",
    "    else:\n",
    "        return welcome\n",
    "\n",
    "a = hi()\n",
    "print(a)\n",
    "#outputs: <function greet at 0x7f2143c01500>\n",
    "\n",
    "#This clearly shows that `a` now points to the greet() function in hi()\n",
    "#Now try this\n",
    "\n",
    "print(a())\n",
    "#outputs: now you are in the greet() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just take a look at the code again. In the if/else clause we are returning greet and welcome, not greet() and welcome(). Why is that? It’s because when you put a pair of parentheses after it, the function gets executed; whereas if you don’t put parenthesis after it, then it can be passed around and can be assigned to other variables without executing it. Did you get it? Let me explain it in a little bit more detail. When we write a = hi(), hi() gets executed and because the name is yasoob by default, the function greet is returned. If we change the statement to a = hi(name = \"ali\") then the welcome function will be returned. We can also do print hi()() which outputs now you are in the greet() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9.4 Giving a function as an argument to another function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hi():\n",
    "    return \"hi yasoob!\"\n",
    "\n",
    "def doSomethingBeforeHi(func):\n",
    "    print(\"I am doing some boring work before executing hi()\")\n",
    "    print(func())\n",
    "\n",
    "doSomethingBeforeHi(hi)\n",
    "#outputs:I am doing some boring work before executing hi()\n",
    "#        hi yasoob!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have all the required knowledge to learn what decorators really are. Decorators let you execute code before and after a function.\n",
    "\n",
    "### 1.9.5. Writing your first decorator:\n",
    "In the last example we actually made a decorator! Let’s modify the previous decorator and make a little bit more usable program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_new_decorator(a_func):\n",
    "\n",
    "    def wrapTheFunction():\n",
    "        print(\"I am doing some boring work before executing a_func()\")\n",
    "\n",
    "        a_func()\n",
    "\n",
    "        print(\"I am doing some boring work after executing a_func()\")\n",
    "\n",
    "    return wrapTheFunction\n",
    "\n",
    "def a_function_requiring_decoration():\n",
    "    print(\"I am the function which needs some decoration to remove my foul smell\")\n",
    "\n",
    "a_function_requiring_decoration()\n",
    "#outputs: \"I am the function which needs some decoration to remove my foul smell\"\n",
    "\n",
    "a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)\n",
    "#now a_function_requiring_decoration is wrapped by wrapTheFunction()\n",
    "\n",
    "a_function_requiring_decoration()\n",
    "#outputs:I am doing some boring work before executing a_func()\n",
    "#        I am the function which needs some decoration to remove my foul smell\n",
    "#        I am doing some boring work after executing a_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you get it? We just applied the previously learned principles. This is exactly what the decorators do in Python! They wrap a function and modify its behaviour in one way or another. Now you might be wondering why we did not use the @ anywhere in our code? That is just a short way of making up a decorated function. Here is how we could have run the previous code sample using @."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@a_new_decorator\n",
    "def a_function_requiring_decoration():\n",
    "    \"\"\"Hey you! Decorate me!\"\"\"\n",
    "    print(\"I am the function which needs some decoration to \"\n",
    "          \"remove my foul smell\")\n",
    "\n",
    "a_function_requiring_decoration()\n",
    "#outputs: I am doing some boring work before executing a_func()\n",
    "#         I am the function which needs some decoration to remove my foul smell\n",
    "#         I am doing some boring work after executing a_func()\n",
    "\n",
    "#the @a_new_decorator is just a short way of saying:\n",
    "a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you now have a basic understanding of how decorators work in Python. Now there is one problem with our code. If we run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a_function_requiring_decoration.__name__)\n",
    "# Output: wrapTheFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s not what we expected! Its name is “a_function_requiring_decoration”. Well, our function was replaced by wrapTheFunction. It overrode the name and docstring of our function. Luckily, Python provides us a simple function to solve this problem and that is functools.wraps. Let’s modify our previous example to use functools.wraps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def a_new_decorator(a_func):\n",
    "    @wraps(a_func)\n",
    "    def wrapTheFunction():\n",
    "        print(\"I am doing some boring work before executing a_func()\")\n",
    "        a_func()\n",
    "        print(\"I am doing some boring work after executing a_func()\")\n",
    "    return wrapTheFunction\n",
    "\n",
    "@a_new_decorator\n",
    "def a_function_requiring_decoration():\n",
    "    \"\"\"Hey yo! Decorate me!\"\"\"\n",
    "    print(\"I am the function which needs some decoration to \"\n",
    "          \"remove my foul smell\")\n",
    "\n",
    "print(a_function_requiring_decoration.__name__)\n",
    "# Output: a_function_requiring_decoration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that is much better. Let’s move on and learn some use-cases of decorators.\n",
    "\n",
    "#### Blueprint:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "def decorator_name(f):\n",
    "    @wraps(f)\n",
    "    def decorated(*args, **kwargs):\n",
    "        if not can_run:\n",
    "            return \"Function will not run\"\n",
    "        return f(*args, **kwargs)\n",
    "    return decorated\n",
    "\n",
    "@decorator_name\n",
    "def func():\n",
    "    return(\"Function is running\")\n",
    "\n",
    "can_run = True\n",
    "print(func())\n",
    "# Output: Function is running\n",
    "\n",
    "can_run = False\n",
    "print(func())\n",
    "# Output: Function will not run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: @wraps takes a function to be decorated and adds the functionality of copying over the function name, docstring, arguments list, etc. This allows us to access the pre-decorated function’s properties in the decorator.\n",
    "\n",
    "#### Authorization\n",
    "Decorators can help to check whether someone is authorized to use an endpoint in a web application. They are extensively used in Flask web framework and Django. Here is an example to employ decorator based authentication:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def requires_auth(f):\n",
    "    @wraps(f)\n",
    "    def decorated(*args, **kwargs):\n",
    "        auth = request.authorization\n",
    "        if not auth or not check_auth(auth.username, auth.password):\n",
    "            authenticate()\n",
    "        return f(*args, **kwargs)\n",
    "    return decorated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def logit(func):\n",
    "    @wraps(func)\n",
    "    def with_logging(*args, **kwargs):\n",
    "        print(func.__name__ + \" was called\")\n",
    "        return func(*args, **kwargs)\n",
    "    return with_logging\n",
    "\n",
    "@logit\n",
    "def addition_func(x):\n",
    "   \"\"\"Do some math.\"\"\"\n",
    "   return x + x\n",
    "\n",
    "\n",
    "result = addition_func(4)\n",
    "# Output: addition_func was called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9.6. Decorators with Arguments\n",
    "Come to think of it, isn’t @wraps also a decorator? But, it takes an argument like any normal function can do. So, why can’t we do that too?\n",
    "\n",
    "This is because when you use the @my_decorator syntax, you are applying a wrapper function with a single function as a parameter. Remember, everything in Python is an object, and this includes functions! With that in mind, we can write a function that returns a wrapper function.\n",
    "\n",
    "#### 1.9.6.1. Nesting a Decorator Within a Function\n",
    "Let’s go back to our logging example, and create a wrapper which lets us specify a logfile to output to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def logit(logfile='out.log'):\n",
    "    def logging_decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapped_function(*args, **kwargs):\n",
    "            log_string = func.__name__ + \" was called\"\n",
    "            print(log_string)\n",
    "            # Open the logfile and append\n",
    "            with open(logfile, 'a') as opened_file:\n",
    "                # Now we log to the specified logfile\n",
    "                opened_file.write(log_string + '\\n')\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapped_function\n",
    "    return logging_decorator\n",
    "\n",
    "@logit()\n",
    "def myfunc1():\n",
    "    pass\n",
    "\n",
    "myfunc1()\n",
    "# Output: myfunc1 was called\n",
    "# A file called out.log now exists, with the above string\n",
    "\n",
    "@logit(logfile='func2.log')\n",
    "def myfunc2():\n",
    "    pass\n",
    "\n",
    "myfunc2()\n",
    "# Output: myfunc2 was called\n",
    "# A file called func2.log now exists, with the above string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9.6.2. Decorator Classes\n",
    "Now we have our logit decorator in production, but when some parts of our application are considered critical, failure might be something that needs more immediate attention. Let’s say sometimes you want to just log to a file. Other times you want an email sent, so the problem is brought to your attention, and still keep a log for your own records. This is a case for using inheritence, but so far we’ve only seen functions being used to build decorators.\n",
    "\n",
    "Luckily, classes can also be used to build decorators. So, let’s rebuild logit as a class instead of a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logit(object):\n",
    "\n",
    "    _logfile = 'out.log'\n",
    "\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        log_string = self.func.__name__ + \" was called\"\n",
    "        print(log_string)\n",
    "        # Open the logfile and append\n",
    "        with open(self._logfile, 'a') as opened_file:\n",
    "            # Now we log to the specified logfile\n",
    "            opened_file.write(log_string + '\\n')\n",
    "        # Now, send a notification\n",
    "        self.notify()\n",
    "\n",
    "        # return base func\n",
    "        return self.func(*args)\n",
    "\n",
    "\n",
    "\n",
    "    def notify(self):\n",
    "        # logit only logs, no more\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation has an additional advantage of being much cleaner than the nested function approach, and wrapping a function still will use the same syntax as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit._logfile = 'out2.log' # if change log file\n",
    "@logit\n",
    "def myfunc1():\n",
    "    pass\n",
    "\n",
    "myfunc1()\n",
    "# Output: myfunc1 was called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s subclass logit to add email functionality (though this topic will not be covered here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class email_logit(logit):\n",
    "    '''\n",
    "    A logit implementation for sending emails to admins\n",
    "    when the function is called.\n",
    "    '''\n",
    "    def __init__(self, email='admin@myproject.com', *args, **kwargs):\n",
    "        self.email = email\n",
    "        super(email_logit, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def notify(self):\n",
    "        # Send an email to self.email\n",
    "        # Will not be implemented here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, @email_logit works just like @logit but sends an email to the admin in addition to logging.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Numpy\n",
    "## 2.1 Broadcasting\n",
    "The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefficient use of memory that slows computation.\n",
    "\n",
    "NumPy operations are usually done on pairs of arrays on an element-by-element basis. In the simplest case, the two arrays must have exactly the same shape, as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = np.array([2.0, 2.0, 2.0])\n",
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy’s broadcasting rule relaxes this constraint when the arrays’ shapes meet certain constraints. The simplest broadcasting example occurs when an array and a scalar value are combined in an operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = 2.0\n",
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is equivalent to the previous example where b was an array. We can think of the scalar b being stretched during the arithmetic operation into an array with the same shape as a. The new elements in b, as shown in Figure 1, are simply copies of the original scalar. The stretching analogy is only conceptual. NumPy is smart enough to use the original scalar value without actually making copies so that broadcasting operations are as memory and computationally efficient as possible.\n",
    "\n",
    "![BC](./assets/broadcasting_1.png)\n",
    "\n",
    "The code in the second example is more efficient than that in the first because broadcasting moves less memory around during the multiplication (b is a scalar rather than an array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 General Broadcasting Rules\n",
    "When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when\n",
    "\n",
    "1. they are equal, or\n",
    "\n",
    "2. one of them is 1\n",
    "\n",
    "If these conditions are not met, a ValueError: operands could not be broadcast together exception is thrown, indicating that the arrays have incompatible shapes. The size of the resulting array is the size that is not 1 along each axis of the inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Broadcastable arrays\n",
    "A set of arrays is called “broadcastable” to the same shape if the above rules produce a valid result.\n",
    "\n",
    "For example, if a.shape is (5,1), b.shape is (1,6), c.shape is (6,) and d.shape is () so that d is a scalar, then a, b, c, and d are all broadcastable to dimension (5,6); and\n",
    "\n",
    "- a acts like a (5,6) array where a[:,0] is broadcast to the other columns,\n",
    "\n",
    "- b acts like a (5,6) array where b[0,:] is broadcast to the other rows,\n",
    "\n",
    "- c acts like a (1,6) array and therefore like a (5,6) array where c[:] is broadcast to every row, and finally,\n",
    "\n",
    "- d acts like a (5,6) array where the single value is repeated.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Pandas\n",
    "\n",
    "## 3.1 Merge\n",
    "\n",
    "## 3.2 Groupby\n",
    "\n",
    "## 3.3 Pivot Table\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to store passenger data of the Titanic. For a number of passengers, I know the name (characters), age (integers) and sex (male/female) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Name\": [\n",
    "            \"Braund, Mr. Owen Harris\",\n",
    "            \"Allen, Mr. William Henry\",\n",
    "            \"Bonnell, Miss. Elizabeth\",\n",
    "        ],\n",
    "        \"Age\": [22, 35, 58],\n",
    "        \"Sex\": [\"male\", \"male\", \"female\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually store data in a table, create a DataFrame. When using a Python dictionary of lists, the dictionary keys will be used as column headers and the values in each list as columns of the DataFrame.\n",
    "\n",
    "<!-- <img src=\"./assets/df.svg\" style=\"zoom:70%\" /> -->\n",
    "![df](./assets/df.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each column in a DataFrame is a Series\n",
    "\n",
    "![series](./assets/01_table_series.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I’m just interested in working with the data in the column Age\n",
    "df[\"Age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When selecting a single column of a pandas DataFrame, the result is a pandas Series. To select the column, use the column label in between square brackets []."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a Series from scratch as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = pd.Series([22, 35, 58], name=\"Age\")\n",
    "\n",
    "ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pandas Series has no column labels, as it is just a single column of a DataFrame. A Series does have row labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The describe() method provides a quick overview of the numerical data in a DataFrame. As the Name and Sex columns are textual data, these are by default not taken into account by the describe() method.\n",
    "\n",
    "Many pandas operations return a DataFrame or a Series. The describe() method is an example of a pandas operation returning a pandas Series or a pandas DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "dtypes is an attribute of a DataFrame and Series.\n",
    "\n",
    "---\n",
    "\n",
    "DataFrame.shape is an attribute of a pandas Series and DataFrame containing the number of rows and columns: (nrows, ncolumns). A pandas Series is 1-dimensional and only the number of rows is returned.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 How do I filter specific rows from a DataFrame?\n",
    "I’m interested in the passengers older than 35 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"./assets/titanic.csv\")\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "above_35 = titanic[titanic[\"Age\"] > 35]\n",
    "\n",
    "above_35.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select rows based on a conditional expression, use a condition inside the selection brackets [].\n",
    "\n",
    "The condition inside the selection brackets titanic[\"Age\"] > 35 checks for which rows the Age column has a value larger than 35:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Age\"] > 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the conditional expression (>, but also ==, !=, <, <=,… would work) is actually a pandas Series of boolean values (either True or False) with the same number of rows as the original DataFrame. Such a Series of boolean values can be used to filter the DataFrame by putting it in between the selection brackets []. Only rows for which the value is True will be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’m interested in the Titanic passengers from cabin class 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_23 = titanic[titanic[\"Pclass\"].isin([2, 3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the conditional expression, the isin() conditional function returns a True for each row the values are in the provided list. To filter the rows based on such a function, use the conditional function inside the selection brackets []. In this case, the condition inside the selection brackets titanic[\"Pclass\"].isin([2, 3]) checks for which rows the Pclass column is either 2 or 3.\n",
    "\n",
    "The above is equivalent to filtering by rows for which the class is either 2 or 3 and combining the two statements with an | (or) operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_23 = titanic[(titanic[\"Pclass\"] == 2) | (titanic[\"Pclass\"] == 3)]\n",
    "\n",
    "class_23.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When combining multiple conditional statements, each condition must be surrounded by parentheses (). Moreover, you can not use or/and but need to use the or operator | and the and operator &."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to work with passenger data for which the age is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_no_na = titanic[titanic[\"Age\"].notna()]\n",
    "\n",
    "age_no_na.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notna() conditional function returns a True for each row the values are not an Null value. As such, this can be combined with the selection brackets [] to filter the data table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 How do I select specific rows and columns from a DataFrame?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’m interested in the names of the passengers older than 35 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_names = titanic.loc[titanic[\"Age\"] > 35, \"Name\"]\n",
    "\n",
    "adult_names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a subset of both rows and columns is made in one go and just using selection brackets [] is not sufficient anymore. The loc/iloc operators are required in front of the selection brackets []. When using loc/iloc, the part before the comma is the rows you want, and the part after the comma is the columns you want to select.\n",
    "\n",
    "When using the column names, row labels or a condition expression, use the loc operator in front of the selection brackets []. For both the part before and after the comma, you can use a single label, a list of labels, a slice of labels, a conditional expression or a colon. Using a colon specifies you want to select all rows or columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’m interested in rows 10 till 25 and columns 3 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.iloc[9:25, 2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a subset of both rows and columns is made in one go and just using selection brackets [] is not sufficient anymore. When specifically interested in certain rows and/or columns based on their position in the table, use the iloc operator in front of the selection brackets []."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When selecting specific rows and/or columns with loc or iloc, new values can be assigned to the selected data. For example, to assign the name anonymous to the first 3 elements of the third column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.iloc[0:3, 3] = \"anonymous\"\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 How to create plots in pandas?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality = pd.read_csv(\"./assets/air_quality_no2.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With a DataFrame, pandas creates by default one line plot for each of the columns with numeric data.\n",
    "\n",
    "I want to plot only the columns of the data table with the data from Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality[\"station_paris\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot a specific column, use the selection method of we talked above in combination with the plot() method. Hence, the plot() method works on both Series and DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to visually compare the NO2 values measured in London versus Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality.plot.scatter(x=\"station_london\", y=\"station_paris\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the default line plot when using the plot function, a number of alternatives are available to plot data. Let’s use some standard Python to get an overview of the available plot methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    method_name\n",
    "    for method_name in dir(air_quality.plot)\n",
    "    if not method_name.startswith(\"_\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many development environments as well as IPython and Jupyter Notebook, use the TAB button to get an overview of the available methods, for example air_quality.plot. + TAB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want each of the columns in a separate subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = air_quality.plot.area(figsize=(12, 4), subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate subplots for each of the data columns are supported by the subplots argument of the plot functions. The builtin options available in each of the pandas plot functions are worth reviewing.\n",
    "\n",
    "I want to further customize, extend or save the resulting plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality.plot.area(ax=axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs.set_ylabel(\"NO$_2$ concentration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"./assets/no2_concentrations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the plot objects created by pandas is a matplotlib object. As Matplotlib provides plenty of options to customize plots, making the link between pandas and Matplotlib explicit enables all the power of matplotlib to the plot. This strategy is applied in the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(12, 4))        # Create an empty matplotlib Figure and Axes\n",
    "air_quality.plot.area(ax=axs)                   # Use pandas to put the area plot on the prepared Figure/Axes\n",
    "axs.set_ylabel(\"NO$_2$ concentration\")          # Do any matplotlib customization you like\n",
    "fig.savefig(\"./assets/no2_concentrations.png\")           # Save the Figure/Axes using the existing matplotlib method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 How to create new columns derived from existing columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to express the NO2 concentration of the station in London in mg/m3\n",
    "\n",
    "(If we assume temperature of 25 degrees Celsius and pressure of 1013 hPa, the conversion factor is 1.882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality[\"london_mg_per_cubic\"] = air_quality[\"station_london\"] * 1.882\n",
    "\n",
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new column, use the [] brackets with the new column name at the left side of the assignment.\n",
    "\n",
    "The calculation of the values is done element_wise. This means all values in the given column are multiplied by the value 1.882 at once. You do not need to use a loop to iterate each of the rows!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to check the ratio of the values in Paris versus Antwerp and save the result in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality[\"ratio_paris_antwerp\"] = (\n",
    "    air_quality[\"station_paris\"] / air_quality[\"station_antwerp\"]\n",
    ")\n",
    "\n",
    "\n",
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation is again element-wise, so the / is applied for the values in each row.\n",
    "\n",
    "Also other mathematical operators (+, -, \\*, /) or logical operators (<, >, =,…) work element wise. The latter was already used in the subset data tutorial to filter rows of a table using a conditional expression.\n",
    "\n",
    "If you need more advanced logic, you can use arbitrary Python code via apply()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to rename the data columns to the corresponding station identifiers used by openAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_renamed = air_quality.rename(\n",
    "    columns={\n",
    "        \"station_antwerp\": \"BETR801\",\n",
    "        \"station_paris\": \"FR04014\",\n",
    "        \"station_london\": \"London Westminster\",\n",
    "    }\n",
    ")\n",
    "\n",
    "air_quality_renamed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rename() function can be used for both row labels and column labels. Provide a dictionary with the keys the current names and the values the new names to update the corresponding names.\n",
    "\n",
    "The mapping should not be restricted to fixed names only, but can be a mapping function as well. For example, converting the column names to lowercase letters can be done using a function as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_renamed = air_quality_renamed.rename(columns=str.lower)\n",
    "\n",
    "air_quality_renamed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 How to calculate summary statistics?\n",
    "The statistic applied to multiple columns of a DataFrame (the selection of two columns return a DataFrame, see the subset data tutorial) is calculated for each numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"./assets/titanic.csv\")\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the predefined statistics, specific combinations of aggregating statistics for given columns can be defined using the DataFrame.agg() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.agg(\n",
    "    {\n",
    "        \"Age\": [\"min\", \"max\", \"median\", \"skew\"],\n",
    "        \"Fare\": [\"min\", \"max\", \"median\", \"mean\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Aggregating statistics grouped by category\n",
    "What is the average age for male versus female Titanic passengers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[[\"Sex\", \"Age\"]].groupby(\"Sex\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our interest is the average age for each gender, a subselection on these two columns is made first: titanic[[\"Sex\", \"Age\"]]. Next, the groupby() method is applied on the Sex column to make a group per category. The average age for each gender is calculated and returned.\n",
    "\n",
    "Calculating a given statistic (e.g. mean age) for each category in a column (e.g. male/female in the Sex column) is a common pattern. The groupby method is used to support this type of operations. More general, this fits in the more general split-apply-combine pattern:\n",
    "\n",
    "- Split the data into groups\n",
    "\n",
    "- Apply a function to each group independently\n",
    "\n",
    "- Combine the results into a data structure\n",
    "\n",
    "The apply and combine steps are typically done together in pandas.\n",
    "\n",
    "In the previous example, we explicitly selected the 2 columns first. If not, the mean method is applied to each column containing numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(\"Sex\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not make much sense to get the average value of the Pclass. if we are only interested in the average age for each gender, the selection of columns (rectangular brackets [] as usual) is supported on the grouped data as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(\"Sex\")[\"Age\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gpby](./assets/06_groupby_select_detail.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the mean ticket fare price for each of the sex and cabin class combinations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby([\"Sex\", \"Pclass\"])[\"Fare\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping can be done by multiple columns at the same time. Provide the column names as a list to the groupby() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Count number of records by category\n",
    "![cnrc](./assets/06_valuecounts.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the number of passengers in each of the cabin classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Pclass\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value_counts() method counts the number of records for each category in a column.\n",
    "\n",
    "The function is a shortcut, as it is actually a groupby operation in combination with counting of the number of records within each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(\"Pclass\")[\"Pclass\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both size and count can be used in combination with groupby. Whereas size includes NaN values and just provides the number of rows (size of the table), count excludes the missing values. In the value_counts method, use the dropna argument to include or exclude the NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 How to reshape the layout of tables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality = pd.read_csv(\n",
    "    \"./assets/air_quality_long.csv\", index_col=\"date.utc\", parse_dates=True\n",
    ")\n",
    "\n",
    "\n",
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.1 Sort table rows\n",
    "\n",
    "I want to sort the Titanic data according to the age of the passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.sort_values(by=\"Age\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to sort the Titanic data according to the cabin class and age in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.sort_values(by=['Pclass', 'Age'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Series.sort_values(), the rows in the table are sorted according to the defined column(s). The index will follow the row order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2 Long to wide table format\n",
    "\n",
    "Let’s use a small subset of the air quality data set. We focus on NO2 data and only use the first two measurements of each location (i.e. the head of each group). The subset of data will be called no2_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for no2 data only\n",
    "no2 = air_quality[air_quality[\"parameter\"] == \"no2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 2 measurements (head) for each location (groupby)\n",
    "no2_subset = no2.sort_index().groupby([\"location\"]).head(2) # understand the split-apply-combine pattern\n",
    "\n",
    "no2_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_subset.T # just for fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want the values for the three stations as separate columns next to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_subset.pivot(columns=\"location\", values=\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pivot() function is purely reshaping of the data: a single value for each index/column combination is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As pandas support plotting of multiple columns out of the box, the conversion from long to wide table format enables the plotting of the different time series at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2.pivot(columns=\"location\", values=\"value\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the index parameter is not defined, the existing index (row labels) is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.3 Pivot table\n",
    "![pivot_table](./assets/07_pivot_table.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want the mean concentrations for NO2 and PM2.5 in each of the stations in table form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality.pivot_table(\n",
    "    values=\"value\", index=\"location\", columns=\"parameter\", aggfunc=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of pivot(), the data is only rearranged. When multiple values need to be aggregated (in this specific case, the values on different time steps) pivot_table() can be used, providing an aggregation function (e.g. mean) on how to combine these values.\n",
    "\n",
    "Pivot table is a well known concept in spreadsheet software. When interested in summary columns for each variable separately as well, put the margin parameter to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality.pivot_table(\n",
    "    values=\"value\",\n",
    "    index=\"location\",\n",
    "    columns=\"parameter\",\n",
    "    aggfunc=\"mean\",\n",
    "    margins=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are wondering, pivot_table() is indeed directly linked to groupby(). The same result can be derived by grouping on both parameter and location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality.groupby([\"parameter\", \"location\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.4 Wide to long format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting again from the wide format table created in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_pivoted = no2.pivot(columns=\"location\", values=\"value\").reset_index()\n",
    "\n",
    "no2_pivoted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to collect all air quality NO2 measurements in a single column (long format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_2 = no2_pivoted.melt(id_vars=\"date.utc\")\n",
    "\n",
    "no_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas.melt() method on a DataFrame converts the data table from wide format to long format. The column headers become the variable names in a newly created column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is the short version on how to apply pandas.melt(). The method will melt all columns NOT mentioned in id_vars together into two columns: A column with the column header names and a column with the values itself. The latter column gets by default the name value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas.melt() method can be defined in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_2 = no2_pivoted.melt(\n",
    "    id_vars=\"date.utc\",\n",
    "    value_vars=[\"BETR801\", \"FR04014\", \"London Westminster\"],\n",
    "    value_name=\"NO_2\",\n",
    "    var_name=\"id_location\",\n",
    ")\n",
    "\n",
    "\n",
    "no_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result in the same, but in more detail defined:\n",
    "\n",
    "- value_vars defines explicitly which columns to melt together\n",
    "\n",
    "- value_name provides a custom column name for the values column instead of the default column name value\n",
    "\n",
    "- var_name provides a custom column name for the column collecting the column header names. Otherwise it takes the index name or a default variable\n",
    "\n",
    "Hence, the arguments value_name and var_name are just user-defined names for the two generated columns. The columns to melt are defined by id_vars and value_vars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 How to combine data from multiple tables?\n",
    "![concat](./assets/08_concat_row.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_no2 = pd.read_csv(\"./assets/air_quality_no2_long.csv\",\n",
    "                              parse_dates=True)\n",
    "\n",
    "\n",
    "air_quality_no2 = air_quality_no2[[\"date.utc\", \"location\",\n",
    "                                   \"parameter\", \"value\"]]\n",
    "\n",
    "\n",
    "air_quality_no2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_pm25 = pd.read_csv(\"./assets/air_quality_pm25_long.csv\",\n",
    "                               parse_dates=True)\n",
    "\n",
    "\n",
    "air_quality_pm25 = air_quality_pm25[[\"date.utc\", \"location\",\n",
    "                                     \"parameter\", \"value\"]]\n",
    "\n",
    "\n",
    "air_quality_pm25.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.1 Concatenating objects\n",
    "I want to combine the measurements of NO2 and PM2.5, two tables with a similar structure, in a single table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality = pd.concat([air_quality_pm25, air_quality_no2], axis=0)\n",
    "\n",
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concat() function performs concatenation operations of multiple tables along one of the axis (row-wise or column-wise).\n",
    "\n",
    "By default concatenation is along axis 0, so the resulting table combines the rows of the input tables. Let’s check the shape of the original and the concatenated tables to verify the operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of the ``air_quality_pm25`` table: ', air_quality_pm25.shape)\n",
    "\n",
    "print('Shape of the ``air_quality_no2`` table: ', air_quality_no2.shape)\n",
    "\n",
    "print('Shape of the resulting ``air_quality`` table: ', air_quality.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the resulting table has 3178 = 1110 + 2068 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The axis argument will return in a number of pandas methods that can be applied along an axis. A DataFrame has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1). Most operations like concatenation or summary statistics are by default across rows (axis 0), but can be applied across columns as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting the table on the datetime information illustrates also the combination of both tables, with the parameter column defining the origin of the table (either no2 from table air_quality_no2 or pm25 from table air_quality_pm25):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality = air_quality.sort_values(\"date.utc\")\n",
    "\n",
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific example, the parameter column provided by the data ensures that each of the original tables can be identified. This is not always the case. the concat function provides a convenient solution with the keys argument, adding an additional (hierarchical) row index. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=[\"PM25\", \"NO2\"])\n",
    "air_quality_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The existence of multiple row/column indices at the same time has not been mentioned within these tutorials. Hierarchical indexing or MultiIndex is an advanced and powerful pandas feature to analyze higher dimensional data.\n",
    "\n",
    "Multi-indexing is out of scope for this pandas introduction. For the moment, remember that the function reset_index can be used to convert any level of an index to a column, e.g. air_quality.reset_index(level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.2 Join tables using a common identifier\n",
    "![MERGE](./assets/08_merge_left.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_coord = pd.read_csv(\"./assets/air_quality_stations.csv\")\n",
    "\n",
    "stations_coord.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality = pd.merge(air_quality, stations_coord, how=\"left\", on=\"location\")\n",
    "\n",
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the merge() function, for each of the rows in the air_quality table, the corresponding coordinates are added from the air_quality_stations_coord table. Both tables have the column location in common which is used as a key to combine the information. By choosing the left join, only the locations available in the air_quality (left) table, i.e. FR04014, BETR801 and London Westminster, end up in the resulting table. The merge function supports multiple join options similar to database-style operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous example, there is no common column name. However, the parameter column in the air_quality table and the id column in the air_quality_parameters_name both provide the measured variable in a common format. The left_on and right_on arguments are used here (instead of just on) to make the link between the two tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.3 Database-style DataFrame or named Series joining/merging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pd.merge(\n",
    "    left,\n",
    "    right,\n",
    "    how=\"inner\",\n",
    "    on=None,\n",
    "    left_on=None,\n",
    "    right_on=None,\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=True,\n",
    "    suffixes=(\"_x\", \"_y\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return type will be the same as left. If left is a DataFrame or named Series and right is a subclass of DataFrame, the return type will still be DataFrame.\n",
    "\n",
    "merge is a function in the pandas namespace, and it is also available as a DataFrame instance method merge(), with the calling DataFrame being implicitly considered the left object in the join.\n",
    "\n",
    "The related join() method, uses merge internally for the index-on-index (by default) and column(s)-on-index join. If you are joining on index only, you may wish to use DataFrame.join to save yourself some typing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### many-to-many joins: joining columns on columns.\n",
    "\n",
    "It is worth spending some time understanding the result of the many-to-many join case. In SQL / standard relational algebra, if a key combination appears more than once in both tables, the resulting table will have the Cartesian product of the associated data. Here is a very basic example with one unique key combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame(\n",
    "    {\n",
    "        \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"],\n",
    "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "right = pd.DataFrame(\n",
    "    {\n",
    "        \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"],\n",
    "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "result = pd.merge(left, right, on=\"key\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![relation](./assets/merging_merge_on_key.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more complicated example with multiple join keys. Only the keys appearing in left and right are present (the intersection), since how='inner' by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame(\n",
    "    {\n",
    "        \"key1\": [\"K0\", \"K0\", \"K1\", \"K2\"],\n",
    "        \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n",
    "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "right = pd.DataFrame(\n",
    "    {\n",
    "        \"key1\": [\"K0\", \"K1\", \"K1\", \"K2\"],\n",
    "        \"key2\": [\"K0\", \"K0\", \"K0\", \"K0\"],\n",
    "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "result = pd.merge(left, right, on=[\"key1\", \"key2\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![multiple](./assets/merging_merge_on_key_multiple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The how argument to merge specifies how to determine which keys are to be included in the resulting table. If a key combination does not appear in either the left or right tables, the values in the joined table will be NA. Here is a summary of the how options and their SQL equivalent names:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Merge method | SQL Join Name      | Description                                         |\n",
    "| ------------ | ------------------ | --------------------------------------------------- |\n",
    "| `left`       | `LEFT OUTER JOIN`  | Use keys from left frame only                       |\n",
    "| `right`      | `RIGHT OUTER JOIN` | Use keys from right frame only                      |\n",
    "| `outer`      | `FULL OUTER JOIN`  | Use union of keys from both frames                  |\n",
    "| `inner`      | `INNER JOIN`       | Use intersection of keys from both frames           |\n",
    "| `cross`      | `CROSS JOIN`       | Create the cartesian product of rows of both frames |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, how=\"left\", on=[\"key1\", \"key2\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lala](./assets/merging_merge_on_key_left.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, how=\"right\", on=[\"key1\", \"key2\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lll](./assets/merging_merge_on_key_left.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, how=\"outer\", on=[\"key1\", \"key2\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/merging_merge_on_key_outer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, how=\"inner\", on=[\"key1\", \"key2\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/merging_merge_on_key_inner.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, how=\"cross\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/merging_merge_cross.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. About problems in conda source in China\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Pandas Exercise (see materials posted on Canvas)\n",
    "\n",
    "Let's play a game!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa0bcc37cb1d53a5ae18050bdae53d79e821dde74f36703fee7d6ffe11c2adc2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('basicdl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
